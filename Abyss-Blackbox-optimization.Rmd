---
title: "Abyss Blackbox Optimization"
author: "Nivretta Thatra"
date: "October 15, 2016"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "Blackbox-optimization"
author: "Nivretta Thatra"
date: "October 15, 2016"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Project 2 problem statement
- Human genome assembly  
- Shotgun sequencing to contigs is ~24 hours  
- Metric: Contiguity (how long the contigs are) and how accurate the contigs are 
	- We are optimizing for *contiguity*  
- For genome assembly, there is one key parameter we can optimize, and then various other weaker ones.  
- **Paretto Frontier**   
	- Setting the boundaries for the metrics
	
- Response metric   
- Solution function is unknown  
	- Know min and max, scale of values (continuous or discrete?)  
	- *Exploitation Approach*: Start halfway and try a point to the left and to the right, find next best, iterate
	- *Exploration Approach*: Try a bunch of random ranges  
	- *Grid Search Approach* 

#Project 2 goals  
## Evaluate 3 parameter optimization tools for usability and speed
  
- Backend  
	- What types of input parameters (discrete with large/small ranges, continuous, binary)   
	- Make it portable to other commandline tools so optimizer can be told how to launch the tool  
- Results output  
	- Generate target metrics vs parameters plot  
	- Generate Pareto frontier of the target metric and a second metric of interest (contiguity and correctness) likely in R using ggplot   
	- Generate a report of the results of the optimization
  - Write a short report of our experience
  - Post on GitHub pages
  - Possibly submit to a preprint server (bioRxiv, PeerJ, Figshare)
  - Possibly submit for peer review, such as F1000Research Hackathons

##Project 2 Dataset(s) and Optimzers
- Dataset 
  -  a human bacterial artificial chromosome (BAC), using assembler [ABySS](https://github.com/bcgsc/abyss#readme)
  
- Metrics
  - The key metrics are *contiguity* (1) and *correctness* (2 through 4).
    1. contiguity (NG50, N50) and aligned contiguity (NGA50, NA50)
    2. number of breakpoints when aligned to the reference as a proxy for misassemblies
    3. number of mismatched nucleotides when aligned to the reference, Q = -10*log(mismatches / total_aligned)
    4. completeness, number of reference bases covered by aligned contigs divided by number of reference bases
  - We'll be optimizing the NG50 metric (or NGA50 with a reference genome) and reporting (but probably not optimizing) the correctness metrics. 
  - The primary parameter we'll be optimizing is k (a fundamental parameter of nearly all de Bruijn graph assemblers), and there's a bunch other parameters that we can play with (typically thresholds related to expected coverage).

- Optimizers being evaluated  
	- [OPAL](http://pythonoptimizers.github.io/opal/) by @dpo — [Optimization of algorithms with OPAL](http://link.springer.com/article/10.1007/s12532-014-0067-x)
	  - 
	- [SpearMint](https://github.com/hips/spearmint) by @mgelbart — [Predictive Entropy Search for Multi-objective Bayesian Optimization](https://arxiv.org/abs/1511.05467)
	- [ParOpt](https://github.com/sseemayer/ParOpt) by @sseemayer which uses scipy.optimize 
	- Possibly R packages, [a long list](https://cran.r-project.org/web/views/Optimization.html)
	- Possibly Python packages, [like scikit-optimize](https://scikit-optimize.github.io/)

